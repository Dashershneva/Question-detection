{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "try:\n",
    "    %matplotlib inline\n",
    "except NameError:\n",
    "    pass\n",
    "import pylab\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import namedtuple, OrderedDict, defaultdict\n",
    "from io import BytesIO\n",
    "from struct import pack, unpack\n",
    "\n",
    "def info(*args, **kwargs):\n",
    "    print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.2.1\n"
     ]
    }
   ],
   "source": [
    "info(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"model_dash/\"\n",
    "if not os.path.isdir(DIR):\n",
    "    os.mkdir(DIR)\n",
    "    \n",
    "OUT_DIR = DIR\n",
    "DATA_PATH = \"data_ord\"\n",
    "LOGFILE = open(DIR + \"train.log\", \"w\")\n",
    "\n",
    "PLOT = True\n",
    "TEST = True\n",
    "TENSORBOARD = True\n",
    "\n",
    "DATA_PATHS = [\"{}/feat_norm.ark\".format(DATA_PATH)]\n",
    "LABELS_PATHS = [\"{}/index.json\".format(DATA_PATH)]\n",
    "TIMESTAMPS_PATHS = [\"{}/alignments.json\".format(DATA_PATH)]\n",
    "EMBEDDINGS_PATHS = [\"{}/embeddings.ark\".format(DATA_PATH)]\n",
    "\n",
    "MODE_ACOUSTIC = \"acoustic\"\n",
    "MODE_TEXT = \"text\"\n",
    "MODE_HYBRID = \"hybrid\"\n",
    "\n",
    "#MODE = MODE_ACOUSTIC\n",
    "MODE = MODE_HYBRID\n",
    "#MODE = MODE_TEXT\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "NUM_EPOCHES = 20\n",
    "\n",
    "TRAIN_FRACTION = 0.8\n",
    "\n",
    "MODEL_FILENAME = DIR + \"model.chkpt\"\n",
    "OUTPUT_FROZEN_GRAPH = \"graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple(\"Sample\", [\"name\", \"features\", \"label\", \"accented_word\", \"text\"])\n",
    "TimeStamp = namedtuple(\"TimeStamp\", [\"start\", \"stop\", \"text\"])\n",
    "\n",
    "class Data(object):\n",
    "    \"\"\"Class for loading data and batching.\n",
    "    features are of shape (time, dim)\"\"\"\n",
    "    def __init__(self, datapaths, embeddingspaths, labelpaths, timestampspaths, config=dict()):\n",
    "        self._config = Data.set_default_params(config)\n",
    "        self._data, self._feature_ranges, self._has_accents = \\\n",
    "            Data._loaddata(datapaths, embeddingspaths, labelpaths, timestampspaths, self._config)\n",
    "        self._numclasses = max([sample.label for sample in self._data]) + 1\n",
    "        self._maxduration = max([len(sample.features) for sample in self._data])\n",
    "        self._dim = self._data[0].features.shape[-1]\n",
    "        for sample in self._data:\n",
    "            assert sample.features.shape[-1] == self._dim\n",
    "        #self._config[\"mean\"], self._config[\"std\"] = self.get_mean_std()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "    \n",
    "    @property\n",
    "    def numclasses(self):\n",
    "        return self._numclasses\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    @property\n",
    "    def acoustic_range(self):\n",
    "        return self._feature_ranges.get(\"acoustic\", None)\n",
    "    \n",
    "    @property\n",
    "    def embeddings_range(self):\n",
    "        return self._feature_ranges.get(\"embeddings\", None)\n",
    "    \n",
    "    @property\n",
    "    def maxduration(self):\n",
    "        return self._maxduration\n",
    "\n",
    "    @staticmethod\n",
    "    def set_default_params(config):\n",
    "        new_config = {\"label_map\": {b\"declaration\": 0,\n",
    "                                    b\"question\": 1,\n",
    "                                    \"general_question\": 1,\n",
    "                                    \"open_question\": 1}\n",
    "                     }\n",
    "        new_config.update(config)\n",
    "        return new_config\n",
    "    \n",
    "    def split(self, fraction):\n",
    "        data = self._data\n",
    "        del self._data\n",
    "        part1 = copy.deepcopy(self)\n",
    "        part2 = copy.deepcopy(self)\n",
    "        self._data = data\n",
    "        random.seed(777)  # For reproducible results\n",
    "        random.shuffle(data)\n",
    "        nsplit = int(len(data) * fraction)\n",
    "        part1._data = data[:nsplit]\n",
    "        part2._data = data[nsplit:]\n",
    "        return part1, part2\n",
    "    \n",
    "    def sort(self, keys, reverse=False):\n",
    "        data = list(zip(self._data, keys))\n",
    "        self._data = [t[0] for t in sorted(data, key=lambda x: x[1], reverse=reverse)]\n",
    "    \n",
    "    def subsample(self, start, stop):\n",
    "        data = self._data\n",
    "        del self._data\n",
    "        result = copy.deepcopy(self)\n",
    "        self._data = data\n",
    "        result._data = data[start:stop]\n",
    "        return result\n",
    "    \n",
    "    def split_pos_neg(self):\n",
    "        assert self.numclasses == 2, \"Multiclass is not supported\"\n",
    "        data = self._data\n",
    "        data_pos = []\n",
    "        data_neg = []\n",
    "        for sample in data:\n",
    "            if sample.label:\n",
    "                data_pos.append(sample)\n",
    "            else:\n",
    "                data_neg.append(sample)\n",
    "        del self._data\n",
    "        pos = copy.deepcopy(self)\n",
    "        neg = copy.deepcopy(self)\n",
    "        self._data = data\n",
    "        pos._data = data_pos\n",
    "        neg._data = data_neg\n",
    "        return pos, neg\n",
    "    \n",
    "    def append_data(self, data):\n",
    "        self._data = self._data + data._data\n",
    "    \n",
    "    def get_mean_std(self):\n",
    "        means = [np.mean(sample.features, axis=0, keepdims=True) for sample in self._data]\n",
    "        mean = np.mean(means, axis=0)\n",
    "        variances = [np.var(sample.features, axis=0, keepdims=True) for sample in self._data]\n",
    "        var = np.mean(variances, axis=0)\n",
    "        std = np.sqrt(var)\n",
    "        std[std < 1e-6] = 1.0\n",
    "        return mean, std\n",
    "        \n",
    "    def numbatches(self, batchsize):\n",
    "        return int((len(self) - 1) / batchsize) + 1\n",
    "    \n",
    "    def iterbatches(self, batchsize):\n",
    "        random.shuffle(self._data)\n",
    "        for i in range(0, len(self), batchsize):\n",
    "            yield self._make_batch_dict(self._data[i:i + batchsize], self._has_accents, self._config)\n",
    "            \n",
    "    def makedict(self):\n",
    "        return self._make_batch_dict(self._data, self._has_accents, self._config)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _loaddata(datapaths, embeddingspaths, labelpaths, timestampspaths, config):\n",
    "        acoustic_features = dict()\n",
    "        for path in datapaths:\n",
    "            acoustic_features.update(Data._loadark(path))\n",
    "        word_embeddings = dict()\n",
    "        for path in embeddingspaths:\n",
    "            word_embeddings.update(Data._loadark(path))\n",
    "        labels = dict()\n",
    "        for path in labelpaths:\n",
    "            labels.update(Data._loadlabels(path, config))\n",
    "        timestamps = dict()\n",
    "        for path in timestampspaths:\n",
    "            timestamps.update(Data._loadtimestamps(path))\n",
    "        for key, in labels:\n",
    "            #print(\"labels\", type(key))\n",
    "            break\n",
    "        timestamps_new= dict()\n",
    "        for key in timestamps:\n",
    "            timestamps_new[key.decode()] = timestamps[key]\n",
    "            #print(key)\n",
    "            \n",
    "        has_accents = True\n",
    "        feature_ranges = dict()\n",
    "        data = []\n",
    "        keys = list(acoustic_features.keys()) if acoustic_features else list(word_embeddings.keys())\n",
    "        #print('labels:', labels)\n",
    "        for key in keys:\n",
    "            #print(\"key:\", type(key))\n",
    "            if key not in labels:\n",
    "                continue\n",
    "            label, accented_word, accented_text = labels[key]\n",
    "            if label == config[\"label_map\"][\"general_question\"] and accented_word == -1:\n",
    "                info(\"Question {} has no accented word. Skipping\".format(key))\n",
    "                continue\n",
    "            if accented_word is None:\n",
    "                has_accents = False\n",
    "            if key not in timestamps_new:\n",
    "                info(\"No timestamps for {}. Skipping\".format(key))\n",
    "                continue\n",
    "            ts = timestamps_new[key]\n",
    "            num_words = len(ts)\n",
    "            if num_words > 30:\n",
    "                #print(\"Very long:\", key)\n",
    "                continue\n",
    "            if accented_text is not None:\n",
    "                non_sil_words = [w.text for w in ts if w.text != u\"-\"]\n",
    "                if accented_word >= len(non_sil_words):\n",
    "                    info(\"Recognition out of range in {}\".format(key))\n",
    "                    continue\n",
    "                if accented_text != non_sil_words[accented_word]:\n",
    "                    info(\"Recognition mismatch: {} != {} in {}\".format(\n",
    "                        accented_text.encode(\"utf8\"),\n",
    "                        non_sil_words[accented_word].encode(\"utf8\"),\n",
    "                        key))\n",
    "                    continue\n",
    "            words = [w.text for w in ts]\n",
    "            if num_words == 0:\n",
    "                # Fill features later\n",
    "                data.append(Sample(key, None, label, accented_word, words))\n",
    "                continue\n",
    "                \n",
    "            feats_issil = np.array([w.text == u\"-\" for w in ts], dtype=np.float32).reshape((-1, 1))\n",
    "            feats_index = np.arange(num_words, dtype=np.float32).reshape((-1, 1))\n",
    "            feats_duration = np.array([w.stop - w.start for w in ts], dtype=np.float32).reshape((-1, 1))\n",
    "            feats_tempo = np.array([(w.stop - w.start) / float(len(w.text)) for w in ts], dtype=np.float32).reshape((-1, 1))\n",
    "            all_feats = [] #feats_index, feats_issil, feats_duration, feats_tempo]\n",
    "            if acoustic_features:\n",
    "                feats = acoustic_features[key].T\n",
    "                feature_start = sum([f.shape[1] for f in all_feats])\n",
    "                feature_len = feats.shape[1]\n",
    "                all_feats.append(feats)\n",
    "                feature_ranges[\"acoustic\"] = (feature_start, feature_start + feature_len)\n",
    "            if word_embeddings:\n",
    "                word_feats = word_embeddings[key]\n",
    "                feature_start = sum([f.shape[1] for f in all_feats])\n",
    "                feature_len = word_feats.shape[1]\n",
    "                feats = []\n",
    "                sil_feats = np.zeros(word_feats.shape[-1], dtype=np.float32)\n",
    "                i = 0\n",
    "                for w in ts:\n",
    "                    if w.text == u\"-\":\n",
    "                        feats.append(sil_feats)\n",
    "                    else:\n",
    "                        feats.append(word_feats[i])\n",
    "                        i += 1\n",
    "                assert i == len(word_feats) - 1  # Last embedding is for EOF, drop it\n",
    "                feats = np.array(feats)\n",
    "                all_feats.append(feats)\n",
    "                feature_ranges[\"embeddings\"] = (feature_start, feature_start + feature_len)\n",
    "            \n",
    "            #print(key, words)\n",
    "            #print([f.shape for f in all_feats])\n",
    "            \n",
    "            try:\n",
    "                data.append(Sample(key, np.hstack(all_feats), label, accented_word, words))\n",
    "            except Exception:\n",
    "                #print(\"Error appending sample\", key)\n",
    "                continue\n",
    "            dim = data[-1].features.shape[-1]\n",
    "        for i in range(len(data)):\n",
    "            if data[i].features is None:\n",
    "                data[i] = Sample(data[i].name, np.zeros((1, dim), dtype=np.float32),\n",
    "                                 data[i].label, data[i].accented_word, data[i].text)\n",
    "                \n",
    "        return data, feature_ranges, has_accents\n",
    "    \n",
    "    @staticmethod\n",
    "    def _loadmat(fp):\n",
    "        fmt, _, isize1, nrows, isize2, ncols = unpack(\"=3sbbibi\", fp.read(14))\n",
    "        assert fmt == b\"BFM\", fmt\n",
    "        assert isize1 == 4 and isize2 == 4\n",
    "        buf = fp.read(4 * nrows * ncols)\n",
    "        assert len(buf) == 4 * nrows * ncols\n",
    "        mat = np.frombuffer(buf, dtype=\"f4\").reshape((nrows, ncols))\n",
    "        return mat\n",
    "\n",
    "    @staticmethod\n",
    "    def _loadark(fname):\n",
    "        ark = dict()\n",
    "        buf = open(fname, \"rb\").read()\n",
    "        fp = BytesIO(buf)\n",
    "        while fp.tell() < len(buf):\n",
    "            c = fp.tell()\n",
    "            uid = fp.read(buf.index(b\" \", c) - c)\n",
    "            _ = fp.read(2)\n",
    "            ark[uid.decode()] = Data._loadmat(fp)\n",
    "        return ark\n",
    "    \n",
    "    @staticmethod\n",
    "    def _loadlabels(fname, config):\n",
    "        labels = list()\n",
    "        with open(fname, encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                info = json.loads(line)\n",
    "                accented_word = info.get(\"accented_word\", None)\n",
    "                if accented_word is not None and accented_word > 0:\n",
    "                    accented_word -= 1\n",
    "                if accented_word is not None and accented_word >= 0:\n",
    "                    accented_text = info[\"text\"].lower().split()[accented_word]\n",
    "                else:\n",
    "                    accented_text = None\n",
    "                label = config[\"label_map\"][info[\"purpose\"].encode(\"utf8\")]\n",
    "                if label is not None:\n",
    "                    labels.append((info[\"ID\"], (label, accented_word, accented_text)))\n",
    "        return dict(labels)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _loadtimestamps(fname):\n",
    "        timestamps = list()\n",
    "        with open(fname, encoding='utf-8') as fp:\n",
    "            for line in fp:\n",
    "                toks = json.loads(line)\n",
    "                words = [TimeStamp(w[\"start\"], w[\"stop\"], w[\"text\"]) for w in toks[\"words\"]]\n",
    "                timestamps.append((toks[\"ID\"].encode(), words))\n",
    "        return dict(timestamps)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_batch_dict(data, has_accents, config):\n",
    "        def pad(feature, dim):\n",
    "            return np.vstack([feature,\n",
    "                              np.zeros((dim - feature.shape[0], feature.shape[1]),\n",
    "                                       dtype=np.float32)])\n",
    "        names, features, labels, accented_words, texts = zip(*data)\n",
    "        lengths = list(map(len, features))\n",
    "        maxduration = np.max(lengths)\n",
    "        features = map(lambda f: pad(f, maxduration), features)\n",
    "        labels = np.array(labels)\n",
    "        features = np.stack(features, axis=0)\n",
    "        lengths = np.array(lengths)\n",
    "        batch_dict = {\n",
    "            \"features\": features,\n",
    "            \"labels\": labels,\n",
    "            \"lengths\": lengths,\n",
    "            \"names\": names\n",
    "        }\n",
    "        if has_accents:\n",
    "            accented_words = np.array(accented_words)\n",
    "            batch_dict[\"accented_words\"] = accented_words\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(*paths, **kwargs):\n",
    "    train_fraction=kwargs.get(\"train_fraction\", TRAIN_FRACTION)\n",
    "    \n",
    "    paths = list(paths)\n",
    "    if MODE == MODE_HYBRID:\n",
    "        pass\n",
    "    elif MODE == MODE_ACOUSTIC:\n",
    "        paths[1] = []\n",
    "    elif MODE == MODE_TEXT:\n",
    "        paths[0] = []\n",
    "    else:\n",
    "        assert False\n",
    "    \n",
    "    data = Data(*paths)\n",
    "    \n",
    "    assert data.numclasses <= NUM_CLASSES\n",
    "    data_train, data_valid_test = data.split(train_fraction)\n",
    "    data_valid, data_test = data_valid_test.split(0.5)\n",
    "    data_mean, data_std = data_train.get_mean_std()\n",
    "    \n",
    "    labels = np.array([sample.label for sample in data_valid])\n",
    "    for i in range(NUM_CLASSES):\n",
    "        info(\"Class {} prob: {}\".format(i, np.sum(labels == i) / float(len(labels))))\n",
    "    info(\"\")\n",
    "\n",
    "    info(\"Feature dimension:\", data.dim)\n",
    "    info(\"Max duration:\", data.maxduration)\n",
    "    info(\"\")\n",
    "\n",
    "    info(\"Acoustic range:\", data.acoustic_range)\n",
    "    info(\"Embeddings range:\", data.embeddings_range)\n",
    "    info(\"\")\n",
    "\n",
    "    info(\"Train size:\", len(data_train))\n",
    "    info(\"Valid size:\", len(data_valid))\n",
    "    info(\"Test size:\", len(data_test))\n",
    "    info(\"\")\n",
    "    \n",
    "    return data, data_train, data_valid, data_test, data_mean, data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "\n",
      "Class 0 prob: 0.815028901734104\n",
      "Class 1 prob: 0.18497109826589594\n",
      "\n",
      "Feature dimension: 649\n",
      "Max duration: 30\n",
      "\n",
      "Acoustic range: (0, 349)\n",
      "Embeddings range: (349, 649)\n",
      "\n",
      "Train size: 6920\n",
      "Valid size: 865\n",
      "Test size: 866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info(\"Load data\")\n",
    "info(\"\")\n",
    "data, data_train, data_valid, data_test, data_mean, data_std = \\\n",
    "    load_data(DATA_PATHS, EMBEDDINGS_PATHS, LABELS_PATHS, TIMESTAMPS_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractGatesWrapper(tf.nn.rnn_cell.RNNCell):\n",
    "    \"\"\"Output some inner tensors of RNN cell. If tensor_names is None,\n",
    "    than cell output is directly passed\"\"\"\n",
    "    \n",
    "    def __init__(self, cell, tensor_names=None):\n",
    "        super(ExtractGatesWrapper, self).__init__()\n",
    "        self._cell = cell\n",
    "        self._tensor_names = tensor_names\n",
    "        \n",
    "    def zero_state(self, *args, **kwargs):\n",
    "        return self._cell.zero_state(*args, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "        \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        if self._tensor_names is None:\n",
    "            return self._cell.output_size\n",
    "        else:\n",
    "            return [self._cell.output_size] * len(self._tensor_names)\n",
    "    \n",
    "    @property\n",
    "    def num_outputs(self):\n",
    "        assert self._tensor_names is not None\n",
    "        return len(self._tensor_names)\n",
    "    \n",
    "    @property\n",
    "    def output_name(self, i):\n",
    "        assert self._tensor_names is not None\n",
    "        return self._tensor_names[i]\n",
    "        \n",
    "    def __call__(self, inputs, state, scope):\n",
    "        if self._tensor_names is None:\n",
    "            return self._cell(inputs, state, scope)\n",
    "        \n",
    "        class persistent_locals(object):\n",
    "            def __init__(self, func):\n",
    "                self._locals = {}\n",
    "                self.func = func\n",
    "\n",
    "            def __call__(self, *args, **kwargs):\n",
    "                def tracer(frame, event, arg):\n",
    "                    if event=='return':\n",
    "                        self._locals = frame.f_locals.copy()\n",
    "\n",
    "                # tracer is activated on next call, return or exception\n",
    "                sys.setprofile(tracer)\n",
    "                try:\n",
    "                    # trace the function call\n",
    "                    res = self.func(*args, **kwargs)\n",
    "                finally:\n",
    "                    # disable tracer and replace with old one\n",
    "                    sys.setprofile(None)\n",
    "                return res\n",
    "\n",
    "            def clear_locals(self):\n",
    "                self._locals = {}\n",
    "\n",
    "            @property\n",
    "            def locals(self):\n",
    "                return self._locals\n",
    "        \n",
    "        self._cell._set_scope(scope)\n",
    "        with tf.variable_scope(\n",
    "            self._cell._scope, reuse=(self._cell.built or self._cell._reuse)) as scope:\n",
    "            with tf.name_scope(scope.original_name_scope):\n",
    "                cell_call = persistent_locals(self._cell.call)\n",
    "                m, new_state = cell_call(inputs, state)\n",
    "                output_tensors = [cell_call._locals[name] for name in self._tensor_names]\n",
    "        return output_tensors, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(tf.Graph):\n",
    "    \"\"\"Question intonation word-rnn graph\"\"\"\n",
    "    \n",
    "    def __init__(self, config=dict()):\n",
    "        super(Graph, self).__init__()\n",
    "        \n",
    "        self._config = self.set_default_params(config)\n",
    "        self._summary = None\n",
    "        self._run_metadata = None\n",
    "        self._run_metadata_attached = False\n",
    "        \n",
    "        with self.as_default():\n",
    "            self._placeholders = self._make_placeholders(self._config)\n",
    "\n",
    "            # Make graph\n",
    "            self._normalized_features = self._make_preprocessor(self._placeholders[\"features\"], self._config)\n",
    "            self._normalized_features_alt = self._make_preprocessor(self._placeholders[\"features_alt\"], self._config)\n",
    "            \n",
    "            self._output_logits = dict()\n",
    "            self._outputs = dict()\n",
    "            self._var_list = dict()\n",
    "            self._losses = dict()\n",
    "            self._optimizers = dict()\n",
    "            self._loss_to_model = dict()\n",
    "            \n",
    "            self._output_logits[\"purpose\"], self._var_list[\"purpose\"] = self._make_model(\n",
    "                self._placeholders,\n",
    "                self._normalized_features, self._placeholders[\"lengths\"],\n",
    "                self._config, reuse=False)\n",
    "                \n",
    "            self._rnn_gates, _ = self._make_model(\n",
    "                self._placeholders,\n",
    "                self._normalized_features, self._placeholders[\"lengths\"],\n",
    "                self._config, reuse=True, gate_activations=True)\n",
    "            \n",
    "            self._output_logits[\"accent\"], self._var_list[\"accent\"] = self._make_gates_model(\n",
    "                self._placeholders,\n",
    "                self._rnn_gates, self._placeholders[\"lengths\"],\n",
    "                self._config, reuse=False)\n",
    "            self._var_list[\"accent\"].extend(self._var_list[\"purpose\"])\n",
    "            \n",
    "            if self._config[\"pair_loss\"]:\n",
    "                self._output_logits[\"purpose_alt\"], self._var_list[\"purpose_alt\"] = self._make_model(\n",
    "                    self._placeholders,\n",
    "                    self._normalized_features_alt, self._placeholders[\"lengths_alt\"],\n",
    "                    self._config, reuse=True)\n",
    "            \n",
    "            for model_type in self._output_logits:\n",
    "                self._outputs[model_type] = self._make_softmax(self._output_logits[model_type], self._config)\n",
    "            \n",
    "            self._losses[\"xent\"] = self._make_loss_xent(\n",
    "                self._output_logits[\"purpose\"],\n",
    "                self._placeholders[\"labels\"],\n",
    "                self._config)\n",
    "            self._loss_to_model[\"xent\"] = \"purpose\"\n",
    "            \n",
    "            self._losses[\"xent_accent\"] = self._make_loss_xent(\n",
    "                self._output_logits[\"accent\"],\n",
    "                self._expand_accented_words(self._placeholders[\"accented_words\"], tf.shape(self._normalized_features)[1]),\n",
    "                self._config)\n",
    "            self._loss_to_model[\"xent_accent\"] = \"accent\"\n",
    "\n",
    "            if self._config[\"pair_loss\"]:\n",
    "                self._pair_stretch, self._losses[\"pair\"] = self._make_loss_pair(scores_pos=self._output_logits[\"purpose\"],\n",
    "                                                                                scores_neg=self._output_logits[\"purpose_alt\"],\n",
    "                                                                                stretch=self._placeholders[\"pair_loss_stretch\"],\n",
    "                                                                                config=self._config)\n",
    "                self._loss_to_model[\"pair\"] = \"purpose\"\n",
    "\n",
    "            for loss_type, var_list in self._loss_to_model.items():\n",
    "                self._optimizers[loss_type] = self._make_optimizer(self._losses[loss_type],\n",
    "                                                                   self._placeholders[\"learning_rate\"],\n",
    "                                                                   self._var_list[var_list],\n",
    "                                                                   self._config)\n",
    "            \n",
    "            self._savers = dict()\n",
    "            for model_type in self._outputs:\n",
    "                self._savers[model_type] = self._make_saver(self._var_list[model_type], self._config)\n",
    "            \n",
    "            # Init interface\n",
    "            self._input_tensors = [self._placeholders[\"features\"], self._placeholders[\"lengths\"]]\n",
    "            self._output_tensor = self._outputs[\"purpose\"]\n",
    "            self._info_tensors = []\n",
    "            \n",
    "            # Init summaries\n",
    "            \n",
    "            tf.summary.scalar(\"learning_rate\", self._placeholders[\"learning_rate\"])\n",
    "        \n",
    "            self._losses_integrator_input = tf.placeholder(tf.float32, shape=(None, ), name=\"losses_integrator_input\")\n",
    "            self._losses_summaries = dict()\n",
    "            for loss_type in self._losses:\n",
    "                self._losses_summaries[loss_type] = \\\n",
    "                    self._make_loss_summary(self._losses_integrator_input, loss_type, self._config)\n",
    "                    \n",
    "            if self._config[\"pair_loss\"]:\n",
    "                tf.summary.scalar(\"pair_stretch\", self._pair_stretch)\n",
    "            \n",
    "            if config.get(\"summary_path\", None) is not None:\n",
    "                self._merged_summary_op = tf.summary.merge_all()\n",
    "            else:\n",
    "                self._merged_summary_op = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def set_default_params(config=dict()):\n",
    "        new_config = {\"numclasses\": NUM_CLASSES,\n",
    "                      \"featdim\": 553,\n",
    "                      \"select_features\": None,  # list of indices of features to extract\n",
    "                      \"global_mean\": None,\n",
    "                      \"global_std\": None,\n",
    "                      \"normalization\": None,\n",
    "                      \"input_projection_size\": 32,\n",
    "                      \"bottleneck_size\": 16,\n",
    "                      \"gates_fc_layer_size\": 32,\n",
    "                      \"rnn_num_layers\": 1,\n",
    "                      \"rnn_num_units\": 64,\n",
    "                      \"leaky_relu_alpha\": 0.01,\n",
    "                      \"pair_loss\": False,\n",
    "                      \"pair_loss_stretch\": 1.0,\n",
    "                      \"pair_loss_sq_weight\": 0.1,\n",
    "                      \"optimizer\": tf.train.AdamOptimizer,\n",
    "                      \"gradient_clipping\": 5.0,\n",
    "                      \"fc_input_dropout\": True,\n",
    "                      \"make_bottleneck\": True,\n",
    "                      \"summary_path\": None}\n",
    "        for k in config:\n",
    "            assert k in new_config, \"Unknown option {}\".format(k)\n",
    "        new_config.update(config)\n",
    "        return new_config\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_default_train_params(config=dict()):\n",
    "        new_config = {\n",
    "            \"batch_size\": 32,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"rnn_input_keep_prob\": 1.0,\n",
    "            \"rnn_output_keep_prob\": 1.0,\n",
    "            \"rnn_state_keep_prob\": 1.0,\n",
    "            \"pair_sort_margin\": True,\n",
    "            \"pair_max_batches\": 5000\n",
    "        }\n",
    "        new_config.update(config)\n",
    "        return new_config\n",
    "    \n",
    "    def initialize(self):\n",
    "        if self._merged_summary_op is not None:\n",
    "            self._summary_writer = {\n",
    "                \"train\": tf.summary.FileWriter(config[\"summary_path\"] + \"train\", graph=self),\n",
    "                \"valid\": tf.summary.FileWriter(config[\"summary_path\"] + \"valid\", graph=self)\n",
    "            }\n",
    "        session = tf.get_default_session()\n",
    "        self._run_metadata = tf.RunMetadata()\n",
    "        session.run([tf.global_variables_initializer()])\n",
    "        \n",
    "    def _make_epoch(self, data, loss, optimizer, config=dict(), train=True):\n",
    "        pair = (\"pair\" in self._losses and loss == self._losses[\"pair\"])\n",
    "        if pair:\n",
    "            info(\"Eval scores\")\n",
    "            data_pos, data_neg = data.split_pos_neg()\n",
    "            if config[\"pair_sort_margin\"] and train:\n",
    "                info(\"Sort samples for pair loss\")\n",
    "                scores_pos = self.predict(data_pos, model=\"purpose\", logits=True)[:, 1]\n",
    "                scores_neg = self.predict(data_neg, model=\"purpose\", logits=True)[:, 0]\n",
    "                data_pos.sort(scores_pos)\n",
    "                data_neg.sort(scores_neg)\n",
    "        config = self.set_default_train_params(config)\n",
    "        mode = \"Train\" if train else \"Valid\"\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        if pair:\n",
    "            batches_pos = list(data_pos.iterbatches(batch_size))[:-1]\n",
    "            batches_neg = list(data_neg.iterbatches(batch_size))[:-1]\n",
    "            max_batches = int(np.sqrt(config[\"pair_max_batches\"]))\n",
    "            batches_pos = batches_pos[:max_batches]\n",
    "            batches_neg = batches_neg[:max_batches]\n",
    "            num_batches = len(batches_pos) * len(batches_neg)\n",
    "            batches = itertools.product(batches_pos, batches_neg)\n",
    "        else:\n",
    "            num_batches = data.numbatches(batch_size)\n",
    "            batches = data.iterbatches(batch_size)\n",
    "        losses = []\n",
    "        feed_defaults = {k: v for k, v in config.items()\n",
    "                         if k in [\"learning_rate\", \"rnn_input_keep_prob\", \"rnn_output_keep_prob\", \"rnn_state_keep_prob\"]}\n",
    "        feed_dict = dict()\n",
    "        for name, value in feed_defaults.items():\n",
    "            feed_dict[self._placeholders[name]] = config[name]\n",
    "        for i, batch in enumerate(batches):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            if pair:\n",
    "                feed_dict[self._placeholders[\"features\"]] = batch[0][\"features\"]\n",
    "                feed_dict[self._placeholders[\"lengths\"]] = batch[0][\"lengths\"]\n",
    "                feed_dict[self._placeholders[\"features_alt\"]] = batch[1][\"features\"]\n",
    "                feed_dict[self._placeholders[\"lengths_alt\"]] = batch[1][\"lengths\"]\n",
    "            else:\n",
    "                for name in [\"features\", \"labels\", \"lengths\", \"accented_words\"]:\n",
    "                    if name in batch:\n",
    "                        feed_dict[self._placeholders[name]] = batch[name]\n",
    "            if train:\n",
    "                _, step_loss = self._run([optimizer, loss], feed_dict)\n",
    "            else:\n",
    "                step_loss = self._run([loss], feed_dict)[0]\n",
    "            losses.append(step_loss)\n",
    "            info(\"{} batch loss ({} / {}): {}\\r\".format(mode, i + 1, num_batches, step_loss), end=\"\")\n",
    "        info(\"\")\n",
    "        return losses\n",
    "    \n",
    "    def make_epoch(self, i, data, config=dict(), train=True, loss_type=\"xent\", info_mode=None):\n",
    "        \"\"\"Make one epoch of training or just compute loss\n",
    "        info_mode -- if \"train\" or \"valid\" then dump loss into TB summary\"\"\"\n",
    "        assert info_mode in {None, \"train\", \"valid\"}\n",
    "        assert loss_type in self._losses, \"loss {} is not known\".format(loss_type)\n",
    "        losses = self._make_epoch(data, self._losses[loss_type], self._optimizers[loss_type], config, train)\n",
    "        if info_mode is not None and self._merged_summary_op is not None:\n",
    "            kwargs = dict()\n",
    "            if self._run_metadata is not None:\n",
    "                kwargs[\"run_metadata\"] = self._run_metadata\n",
    "            feed_dict = {self._losses_integrator_input: losses}\n",
    "            for name in [\"learning_rate\", \"rnn_input_keep_prob\", \"rnn_output_keep_prob\", \"rnn_state_keep_prob\"]:\n",
    "                feed_dict[self._placeholders[name]] = config[name]\n",
    "            self._summary = self._run([self._merged_summary_op],\n",
    "                                       feed_dict=feed_dict,\n",
    "                                       **kwargs)[0]\n",
    "            if info_mode is not None:\n",
    "                self.dump_summary(i, info_mode)\n",
    "        mean_loss = np.mean(losses)\n",
    "        return mean_loss\n",
    "    \n",
    "    def predict(self, data, logits=False, model=\"purpose\"):\n",
    "        batch = data.makedict()\n",
    "        feed_dict = {self._placeholders[name]: batch[name]\n",
    "                     for name in [\"features\", \"labels\", \"lengths\", \"accented_words\"] if name in batch}\n",
    "        outputs = self._output_logits if logits else self._outputs\n",
    "        return self._run([outputs[model]], feed_dict)[0]\n",
    "    \n",
    "    def dump(self, path, model=\"purpose\"):\n",
    "        \"\"\"Dump checkpoint\"\"\"\n",
    "        session = tf.get_default_session()\n",
    "        self._savers[model].save(session, path)\n",
    "    \n",
    "    def load(self, path, model=\"purpose\"):\n",
    "        \"\"\"Load checkpoint\"\"\"\n",
    "        session = tf.get_default_session()\n",
    "        self._savers[model].restore(session, path)\n",
    "        \n",
    "    def dump_summary(self, i, info_mode):\n",
    "        \"\"\"Dump Tensorboard summary\"\"\"\n",
    "        if self._summary is None:\n",
    "            return\n",
    "        assert self._summary_writer is not None\n",
    "        info(\"Dump summary\", info_mode)\n",
    "        if not self._run_metadata_attached:\n",
    "            self._summary_writer[\"train\"].add_run_metadata(self._run_metadata, \"graph\")\n",
    "            self._summary_writer[\"valid\"].add_run_metadata(self._run_metadata, \"graph\")\n",
    "            self._run_metadata_attached = True\n",
    "        self._summary_writer[info_mode].add_summary(self._summary, i)\n",
    "        self._summary_writer[info_mode].flush()\n",
    "        info(\"Done\")\n",
    "    \n",
    "    def export(self, path):\n",
    "        \"\"\"Export frozen graph for an external usage\"\"\"\n",
    "        graph_def = self.as_graph_def()\n",
    "        ops = self._input_tensors + [self._output_tensor] + self._info_tensors\n",
    "        graph_def = tf.graph_util.convert_variables_to_constants(session, graph_def,\n",
    "                                                                 [t.op.name for t in ops])\n",
    "        tf.train.write_graph(graph_def, DIR, path, as_text=False)\n",
    "        info(\"Exported to\", path)\n",
    "        info(\"Input tensors:\", self._input_tensors)\n",
    "        info(\"Output tensor:\", self._output_tensor)\n",
    "        info(\"Info tensors:\", self._info_tensors)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_placeholders(config):\n",
    "        \"\"\"Alternative feature stream is used for AUC optimization,\n",
    "        when positive samples assigned to features and negative to features_alt\"\"\"\n",
    "        return {\n",
    "            \"features\": tf.placeholder(tf.float32, shape=(None, None, config[\"featdim\"]), name=\"features\"), # batch x time x dim\n",
    "            \"features_alt\": tf.placeholder(tf.float32, shape=(None, None, config[\"featdim\"]), name=\"features_alt\"), # batch x time x dim\n",
    "            \"lengths\": tf.placeholder(tf.int32, shape=(None), name=\"lengths\"), # batch\n",
    "            \"lengths_alt\": tf.placeholder(tf.int32, shape=(None), name=\"lengths_alt\"), # batch\n",
    "            \"labels\": tf.placeholder(tf.int32, shape=(None,), name=\"labels\"),  # batch\n",
    "            \"accented_words\": tf.placeholder(tf.int32, shape=(None,), name=\"accented_words\"),  # batch\n",
    "            \"learning_rate\": tf.placeholder(tf.float32, shape=(), name=\"learning_rate\"),\n",
    "            \"rnn_input_keep_prob\": tf.placeholder_with_default(tf.constant(1.0), tuple(), name=\"rnn_input_keep_prob\"), # Dropout\n",
    "            \"rnn_output_keep_prob\": tf.placeholder_with_default(tf.constant(1.0), tuple(), name=\"rnn_output_keep_prob\"), # Dropout\n",
    "            \"rnn_state_keep_prob\": tf.placeholder_with_default(tf.constant(1.0), tuple(), name=\"rnn_state_keep_prob\"), # Dropout\n",
    "            \"fc_input_keep_prob\": tf.placeholder_with_default(tf.constant(1.0), tuple(), name=\"fc_input_keep_prob\"), # Dropout\n",
    "            \"pair_loss_stretch\": tf.placeholder_with_default(tf.constant(config[\"pair_loss_stretch\"], dtype=tf.float32), tuple(), name=\"pair_loss_stretch\")\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_preprocessor(features, config):\n",
    "        # Global normalization\n",
    "        mean, std = config[\"global_mean\"], config[\"global_std\"]\n",
    "        assert mean is not None\n",
    "        assert std is not None\n",
    "        features = tf.subtract(features, tf.constant(mean, name=\"global_mean\"))\n",
    "        features = tf.divide(features, tf.constant(std, name=\"global_std\"))\n",
    "        \n",
    "        # Per sample normalization\n",
    "        normalization = config[\"normalization\"]\n",
    "        if normalization:\n",
    "            mean, var = tf.nn.moments(features, axes=[1], keep_dims=True)\n",
    "            for name in normalization.split(\",\"):\n",
    "                if name == \"mean\":\n",
    "                    features = tf.subtract(features, mean)\n",
    "                elif name == \"var\":\n",
    "                    std = tf.sqrt(var)\n",
    "                    features = tf.divide(features, std)\n",
    "                else:\n",
    "                    assert False\n",
    "        \n",
    "        # Feature selection\n",
    "        if config[\"select_features\"] is not None:\n",
    "            dim = features.get_shape().as_list()[-1]\n",
    "            info(\"Select {} features out of {}\".format(len(config[\"select_features\"]), dim))\n",
    "            projection = tf.constant(np.identity(dim, dtype=np.float32)[:, config[\"select_features\"]])\n",
    "            features = Graph._batch_matmul(features, projection)\n",
    "            assert features.get_shape().as_list()[-1] == len(config[\"select_features\"])\n",
    "        return features\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_model(placeholders, features, lengths, config, reuse=None, gate_activations=False):\n",
    "        num_layers = config[\"rnn_num_layers\"]\n",
    "        gates_names = None\n",
    "        if gate_activations:\n",
    "            reuse = True\n",
    "            num_layers = 1  # We need activations only for the first layer\n",
    "            gates_names = [\"i\", \"o\", \"f\"]\n",
    "\n",
    "        with tf.variable_scope(\"rnn\", reuse=reuse):\n",
    "            with tf.variable_scope(\"input_projection\", reuse=reuse):\n",
    "                features = Graph._make_projection(features, config[\"input_projection_size\"], config)\n",
    "            cells = []\n",
    "            for i in range(num_layers):\n",
    "                input_dim = features.get_shape()[-1] if i == 0 else config[\"rnn_num_units\"]\n",
    "                with tf.variable_scope(\"rnn_cell{}\".format(i), reuse=reuse):\n",
    "                    cell = Graph._make_rnn_cell(placeholders, input_dim, reuse, config, gates_names=gates_names)\n",
    "                    cells.append(cell)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "            layer_output, layer_state = tf.nn.dynamic_rnn(cell=cell,\n",
    "                                                          inputs=features, \n",
    "                                                          sequence_length=lengths,\n",
    "                                                          dtype=tf.float32,\n",
    "                                                          time_major=False,\n",
    "                                                          swap_memory=True)\n",
    "            if gate_activations:\n",
    "                variables =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"rnn\")\n",
    "                return tf.identity(layer_output, name=\"gates\"), variables\n",
    "            \n",
    "            # layer_output: batch x time x dim\n",
    "            batch_range = tf.range(tf.shape(layer_output)[0])\n",
    "            indices = tf.stack([batch_range, lengths - 1], axis=1)\n",
    "            final_outputs = tf.gather_nd(layer_output, indices)\n",
    "            \n",
    "            if config[\"make_bottleneck\"]:\n",
    "                with tf.variable_scope(\"bottleneck\", reuse=reuse):\n",
    "                    final_outputs = Graph._make_fully_connected(placeholders, final_outputs, config[\"bottleneck_size\"], config)\n",
    "            \n",
    "            with tf.variable_scope(\"output_projection\", reuse=reuse):\n",
    "                output_tensor = Graph._make_projection(final_outputs, config[\"numclasses\"], config)\n",
    "        variables =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"rnn\")\n",
    "        return tf.identity(output_tensor, name=\"logits\"), variables\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_gates_model(placeholders, gates_activations, lengths, config, reuse=None):\n",
    "        with tf.variable_scope(\"gates_model\", reuse=reuse):\n",
    "            gates_activations = gates_activations[0]  # input gate\n",
    "            layer_output = Graph._make_fully_connected(placeholders, gates_activations, config[\"gates_fc_layer_size\"], config)\n",
    "            with tf.variable_scope(\"projection\", reuse=reuse):\n",
    "                output_tensor = Graph._make_projection(layer_output, 2, config)\n",
    "        variables =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"gates_model\")\n",
    "        return output_tensor, variables\n",
    "    \n",
    "    @staticmethod\n",
    "    def _expand_accented_words(accented_words, length):\n",
    "        return tf.one_hot(accented_words, depth=length, dtype=tf.int32)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_projection(input_tensor, num_proj, config):\n",
    "        projection = tf.get_variable(\"projection_matrix\",\n",
    "                                     (input_tensor.get_shape()[-1], num_proj),\n",
    "                                     tf.float32,\n",
    "                                     tf.contrib.layers.xavier_initializer())\n",
    "        return Graph._batch_matmul(input_tensor, projection)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _batch_matmul(a, b):\n",
    "        num_proj = b.get_shape().as_list()[-1]\n",
    "        input_shape = tf.shape(a)\n",
    "        input_flat = tf.reshape(a, [-1, a.get_shape().as_list()[-1]])\n",
    "        output_flat = tf.matmul(input_flat, b)\n",
    "        output_shape = tf.concat([input_shape[:-1], tf.constant(num_proj, shape=[1])], 0)\n",
    "        return tf.reshape(output_flat, output_shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_rnn_cell(placeholders, input_dim, reuse, config, gates_names=None):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units=config[\"rnn_num_units\"], state_is_tuple=True, reuse=reuse)\n",
    "        cell = ExtractGatesWrapper(cell, gates_names)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "                                             input_keep_prob=placeholders[\"rnn_input_keep_prob\"],\n",
    "                                             output_keep_prob=placeholders[\"rnn_output_keep_prob\"],\n",
    "                                             state_keep_prob=placeholders[\"rnn_state_keep_prob\"],\n",
    "                                             variational_recurrent=True,\n",
    "                                             input_size=input_dim,\n",
    "                                             dtype=tf.float32)\n",
    "        return cell\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_rnn_cell_gates(placeholders, input_dim, reuse, config):\n",
    "        cell = LSTMCell(num_units=config[\"rnn_num_units\"], state_is_tuple=True, reuse=reuse)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "                                             input_keep_prob=placeholders[\"rnn_input_keep_prob\"],\n",
    "                                             output_keep_prob=placeholders[\"rnn_output_keep_prob\"],\n",
    "                                             state_keep_prob=placeholders[\"rnn_state_keep_prob\"],\n",
    "                                             variational_recurrent=True,\n",
    "                                             input_size=input_dim,\n",
    "                                             dtype=tf.float32)\n",
    "        return cell\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_convolution(placeholders, input_tensor, dims, config):\n",
    "        prev_size = input_tensor.get_shape().as_list()[-1]\n",
    "        kernel = (dims[1], dims[2], prev_size, dims[0])\n",
    "        kernel_tensor = tf.get_variable(\"weights\", \n",
    "                                        dtype=tf.float32,\n",
    "                                        initializer=tf.truncated_normal(kernel, dtype=tf.float32, stddev=1e-1))\n",
    "        conv = tf.nn.conv2d(input_tensor, kernel_tensor, [1, 1, 1, 1], padding=\"VALID\")\n",
    "        biases = tf.get_variable(\"biases\",\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant(1.0, shape=[kernel[-1]], dtype=tf.float32))\n",
    "        output_tensor = tf.nn.bias_add(conv, biases)\n",
    "        return output_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_pooling(placeholders, input_tensor, dims, config):\n",
    "        kernel = (1, dims[0], dims[1], 1)\n",
    "        output_tensor = tf.nn.max_pool(input_tensor,\n",
    "                                       ksize=kernel,\n",
    "                                       strides=kernel,\n",
    "                                       padding=\"VALID\",\n",
    "                                       name=\"pool\")\n",
    "        return output_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_fully_connected(placeholders, input_tensor, num_bins, config):\n",
    "        if config[\"fc_input_dropout\"]:\n",
    "            input_tensor = tf.nn.dropout(input_tensor, placeholders[\"fc_input_keep_prob\"])\n",
    "        input_shape = tf.shape(input_tensor)\n",
    "        input_dim = input_tensor.get_shape()[-1].value\n",
    "        output_shape = tf.stack([input_shape[i] for i in range(len(input_tensor.get_shape()) - 1)] + [num_bins])\n",
    "        weights = tf.get_variable(\"weights\",\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal([input_dim, num_bins],\n",
    "                                                                  dtype=tf.float32,\n",
    "                                                                  stddev=1e-1))\n",
    "        biases = tf.get_variable(\"biases\",\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant(1.0, shape=[num_bins], dtype=tf.float32))\n",
    "        \n",
    "        input_flat = tf.reshape(input_tensor, [-1, input_dim])\n",
    "        output_flat = tf.nn.bias_add(tf.matmul(input_flat, weights), biases)\n",
    "        return tf.reshape(output_flat, output_shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_leaky_relu(placeholders, input_tensor, dims, config):\n",
    "        alpha = config[\"leaky_relu_alpha\"]\n",
    "        return tf.maximum(alpha * input_tensor, input_tensor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_sigmoid(placeholders, input_tensor, dims, config):\n",
    "        return tf.sigmoid(input_tensor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_softmax(input_tensor, config):\n",
    "        return tf.nn.softmax(input_tensor)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_loss_xent(logits, labels, config):\n",
    "        loss_xent = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "        loss_shift = tf.square(tf.reduce_mean(logits))\n",
    "        return loss_xent + loss_shift\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_loss_mse(outputs, labels, config):\n",
    "        labels = tf.one_hot(labels, config[\"numclasses\"])\n",
    "        return tf.reduce_mean(tf.square(outputs - labels))\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_loss_pair(scores_pos, scores_neg, stretch, config):\n",
    "        \"\"\"ROC AUC optimization\"\"\"\n",
    "        assert scores_pos.get_shape()[-1] == 2, \"Multiclass is not implemented\"\n",
    "        scores_pos = scores_pos[:, 1]\n",
    "        scores_neg = scores_neg[:, 1]\n",
    "        losses_auc = tf.sigmoid((scores_neg - scores_pos) / stretch)\n",
    "        losses_scale = tf.square(scores_neg) + tf.square(scores_pos)\n",
    "        scale_weight = tf.constant(config[\"pair_loss_sq_weight\"])\n",
    "        return stretch, tf.reduce_mean(losses_auc + scale_weight * losses_scale)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_loss_summary(losses, suffix, config):\n",
    "        with tf.name_scope(\"summary\"):\n",
    "            mean_loss = tf.reduce_mean(losses)\n",
    "            tf.summary.scalar(\"loss_\" + suffix, mean_loss)\n",
    "        return mean_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_optimizer(loss, learning_rate, var_list, config):\n",
    "        optimizer = config[\"optimizer\"](learning_rate=learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss, var_list=var_list))\n",
    "        info(\"Optimize:\", [var.name for var in v])\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, config[\"gradient_clipping\"])\n",
    "        return optimizer.apply_gradients(zip(gradients, v))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_saver(var_list, config):\n",
    "        return tf.train.Saver(max_to_keep=5, var_list=var_list)\n",
    "    \n",
    "    def _run(self, tensors, feed_dict=dict(), **kwargs):\n",
    "        assert isinstance(tensors, list) or isinstance(tensors, tuple)\n",
    "        session = tf.get_default_session()\n",
    "        result = session.run(tensors, feed_dict=feed_dict, **kwargs)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize: ['rnn/input_projection/projection_matrix:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0', 'rnn/bottleneck/weights:0', 'rnn/bottleneck/biases:0', 'rnn/output_projection/projection_matrix:0']\n",
      "Optimize: ['gates_model/weights:0', 'gates_model/biases:0', 'gates_model/projection/projection_matrix:0', 'rnn/input_projection/projection_matrix:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0', 'rnn/bottleneck/weights:0', 'rnn/bottleneck/biases:0', 'rnn/output_projection/projection_matrix:0']\n",
      "Optimize: ['rnn/input_projection/projection_matrix:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0', 'rnn/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0', 'rnn/bottleneck/weights:0', 'rnn/bottleneck/biases:0', 'rnn/output_projection/projection_matrix:0']\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"summary_path\": DIR,\n",
    "    \"featdim\": data.dim,\n",
    "    \"select_features\": None,\n",
    "    \"global_mean\": data_mean,\n",
    "    \"global_std\": data_std,\n",
    "    \"pair_loss\": True\n",
    "}\n",
    "graph = Graph(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_point(scores_pos, scores_neg, th):\n",
    "    tp = np.mean(scores_pos > th)\n",
    "    tn = np.mean(scores_neg < th)\n",
    "    fp = 1 - tn\n",
    "    return (th, tp, fp)\n",
    "\n",
    "\n",
    "def get_roc(scores, labels, th_range=(-1.0, 2.0)):\n",
    "    labels = np.array(labels)\n",
    "    scores_pos = scores[labels == 1]\n",
    "    scores_neg = scores[labels == 0]\n",
    "    roc = []\n",
    "    n = [0, 0]\n",
    "    for score, label in sorted(itertools.chain(\n",
    "        [(s, 0) for s in scores_neg],\n",
    "        [(s, 1) for s in scores_pos]), key=lambda x: -x[0]):\n",
    "        n[label] += 1\n",
    "        tp = n[1] / float(len(scores_pos))\n",
    "        fp = n[0] / float(len(scores_neg))\n",
    "        roc.append((score, tp, fp))\n",
    "    return roc\n",
    "\n",
    "\n",
    "def get_auc(roc):\n",
    "    roc = list(sorted(roc, key=lambda x: x[2]))\n",
    "    ths, tps, fps = zip(*roc)\n",
    "    return np.trapz(tps, fps)\n",
    "\n",
    "\n",
    "def plot_roc(roc, positive_rate=None):\n",
    "    ths, tps, fps = zip(*roc)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_aspect(1)\n",
    "    ax.plot(fps, tps)\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", alpha=.5)\n",
    "    ax.plot([1, 0], [0, 1], \"k--\", alpha=.5)\n",
    "    if positive_rate is not None:\n",
    "        x = positive_rate / (1.0 - positive_rate)\n",
    "        ax.plot([0, x], [0, 1], \"-\", color=\"r\", alpha=.5)\n",
    "    ax.set_xlabel(\"False positive\")\n",
    "    ax.set_ylabel(\"True positive\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def split_word_scores(all_scores, data):\n",
    "    assert len(all_scores) == len(data)\n",
    "    scores = []\n",
    "    labels = []\n",
    "    for sample, score in zip(data, all_scores):\n",
    "        for i in range(sample.features.shape[0]):\n",
    "            scores.append(score[i])\n",
    "            labels.append(int(i == sample.accented_word))\n",
    "    return np.array(scores), np.array(labels)\n",
    "    \n",
    "\n",
    "def eval_graph(graph, data, model=\"purpose\"):\n",
    "    scores = graph.predict(data, model=model)\n",
    "    if model == \"accent\":\n",
    "        scores, labels = split_word_scores(scores, data)\n",
    "    else:\n",
    "        labels = np.array([sample.label for sample in data])\n",
    "    true_scores = np.concatenate([scores[labels == cls, cls] for cls in range(NUM_CLASSES)])\n",
    "    false_scores = np.concatenate([scores[labels != cls, cls] for cls in range(NUM_CLASSES)])\n",
    "    cross_entropy = -np.mean(np.log2(np.concatenate([true_scores, 1 - false_scores])))\n",
    "    mse = np.mean(np.square(1 - true_scores))\n",
    "    answers = np.argmax(scores, axis=1)\n",
    "    accuracy = np.mean(answers == labels)\n",
    "    info(\"Accuracy:\", accuracy)\n",
    "    info(\"CrossEntropy:\", cross_entropy)\n",
    "    info(\"MSE:\", mse)\n",
    "    if NUM_CLASSES == 2:\n",
    "        roc = get_roc(scores[:, 1], labels)\n",
    "        auc = get_auc(roc)\n",
    "        info(\"AUC:\", auc)\n",
    "        return auc\n",
    "    else:\n",
    "        return cross_entropy\n",
    "\n",
    "            \n",
    "def train(graph, train_data, test_data, config=dict(), restore=None,\n",
    "          drop_factor=0.1, max_tries=3, max_drops=3, loss_type=None, model=None):\n",
    "    config = graph.set_default_train_params(config)\n",
    "    epoch = 1\n",
    "    num_tries = 0\n",
    "    num_drops = 0\n",
    "    best_loss = None\n",
    "    test_loss = None\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        if restore:\n",
    "            info(\"Load model\")\n",
    "            graph.initialize()\n",
    "            graph.load(MODEL_FILENAME, model=restore)\n",
    "            # TODO: turn on DROPOUT during evaluation\n",
    "            best_loss = graph.make_epoch(0, train_data, config=config, train=False,\n",
    "                                         loss_type=loss_type, info_mode=\"train\")\n",
    "            info(\"Init loss:\", best_loss)\n",
    "            graph.dump(MODEL_FILENAME, model=model)\n",
    "        else:\n",
    "            info(\"Initialize model\")\n",
    "            graph.initialize()\n",
    "            graph.dump(MODEL_FILENAME, model=model)\n",
    "        while num_drops < max_drops:\n",
    "            info(\"Epoch:\", epoch)\n",
    "            epoch += 1\n",
    "            info(\"TEST:\")\n",
    "            eval_graph(graph, test_data, model=model)\n",
    "            test_loss = graph.make_epoch(epoch, test_data, config=config, train=False, loss_type=loss_type,\n",
    "                                         info_mode=\"valid\")\n",
    "            info(\"Test loss:\", test_loss)\n",
    "            info(\"TRAIN:\")\n",
    "            info(\"Learning rate:\", config[\"learning_rate\"])\n",
    "            loss = graph.make_epoch(epoch, train_data, config=config, loss_type=loss_type,\n",
    "                                    info_mode=\"train\")\n",
    "            info(\"Train loss:\", loss)\n",
    "            if best_loss is None or loss < best_loss:\n",
    "                best_loss = loss\n",
    "                info(\"Dump model\", MODEL_FILENAME)\n",
    "                graph.dump(MODEL_FILENAME, model=model)\n",
    "            else:\n",
    "                info(\"Revert\")\n",
    "                graph.load(MODEL_FILENAME, model=model)\n",
    "                num_tries += 1\n",
    "                if num_tries >= max_tries:\n",
    "                    config[\"learning_rate\"] *= drop_factor\n",
    "                    num_tries = 0\n",
    "                    num_drops += 1\n",
    "        info(\"EVAL TRAIN\")\n",
    "        eval_graph(graph, train_data, model=model)\n",
    "        train_loss = graph.make_epoch(epoch, train_data, config=config, train=False, loss_type=loss_type,\n",
    "                                      info_mode=\"valid\")\n",
    "\n",
    "        info(\"EVAL TEST\")\n",
    "        eval_graph(graph, test_data, model=model)\n",
    "        test_loss = graph.make_epoch(epoch, test_data, config=config, train=False, loss_type=loss_type,\n",
    "                                     info_mode=\"valid\")\n",
    "        info(\"Final train loss:\", train_loss)\n",
    "        info(\"Final test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train question prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize model\n",
      "Epoch: 1\n",
      "TEST:\n",
      "Accuracy: 0.208092485549\n",
      "CrossEntropy: 1.38744\n",
      "MSE: 0.375504\n",
      "AUC: 0.515514184397\n",
      "Valid batch loss (14 / 14): 3.2513599395751953\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 3.26923\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.48056766390800476\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.904367\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 2\n",
      "TEST:\n",
      "Accuracy: 0.815028901734\n",
      "CrossEntropy: 0.638541\n",
      "MSE: 0.13958\n",
      "AUC: 0.713820921986\n",
      "Valid batch loss (14 / 14): 0.51926523447036746\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.456566\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.41259878873825073\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.447373\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 3\n",
      "TEST:\n",
      "Accuracy: 0.846242774566\n",
      "CrossEntropy: 0.539629\n",
      "MSE: 0.113269\n",
      "AUC: 0.80725177305\n",
      "Valid batch loss (14 / 14): 0.44142907857894916\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.416861\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.16206820309162147\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.397378\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 4\n",
      "TEST:\n",
      "Accuracy: 0.867052023121\n",
      "CrossEntropy: 0.497859\n",
      "MSE: 0.103176\n",
      "AUC: 0.837588652482\n",
      "Valid batch loss (14 / 14): 0.2542034387588501\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.376168\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.42237755656242373\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.375937\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 5\n",
      "TEST:\n",
      "Accuracy: 0.86936416185\n",
      "CrossEntropy: 0.488826\n",
      "MSE: 0.101723\n",
      "AUC: 0.847393617021\n",
      "Valid batch loss (14 / 14): 0.39659830927848816\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.373761\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.17658922076225285\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.365834\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 6\n",
      "TEST:\n",
      "Accuracy: 0.868208092486\n",
      "CrossEntropy: 0.485899\n",
      "MSE: 0.100023\n",
      "AUC: 0.847792553191\n",
      "Valid batch loss (14 / 14): 0.5341785550117493\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.362554\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.23132325708866124\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.352009\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 7\n",
      "TEST:\n",
      "Accuracy: 0.873988439306\n",
      "CrossEntropy: 0.482101\n",
      "MSE: 0.0983965\n",
      "AUC: 0.852287234043\n",
      "Valid batch loss (14 / 14): 0.24986486136913376\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.358281\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.49086955189704895\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.34915\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 8\n",
      "TEST:\n",
      "Accuracy: 0.867052023121\n",
      "CrossEntropy: 0.47365\n",
      "MSE: 0.0985257\n",
      "AUC: 0.861843971631\n",
      "Valid batch loss (14 / 14): 0.14139424264431757\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.345829\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.47934180498123175\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.343624\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 9\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.491934\n",
      "MSE: 0.101966\n",
      "AUC: 0.855815602837\n",
      "Valid batch loss (14 / 14): 0.33798223733901987\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.340613\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.37816056609153755\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.337584\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 10\n",
      "TEST:\n",
      "Accuracy: 0.873988439306\n",
      "CrossEntropy: 0.47248\n",
      "MSE: 0.097039\n",
      "AUC: 0.85804964539\n",
      "Valid batch loss (14 / 14): 0.22830317914485931\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.3511\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.19681930541992188\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.331059\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 11\n",
      "TEST:\n",
      "Accuracy: 0.864739884393\n",
      "CrossEntropy: 0.486277\n",
      "MSE: 0.1014\n",
      "AUC: 0.862473404255\n",
      "Valid batch loss (14 / 14): 0.20432694256305695\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.337341\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.42958030104637146\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.325039\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 12\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.496444\n",
      "MSE: 0.104094\n",
      "AUC: 0.865407801418\n",
      "Valid batch loss (14 / 14): 0.20376320183277136\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.344944\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.13770657777786255\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.325752\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 13\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.496444\n",
      "MSE: 0.104094\n",
      "AUC: 0.865407801418\n",
      "Valid batch loss (14 / 14): 0.35400480031967163\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.355729\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.25677996873855596\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.331247\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 14\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.496444\n",
      "MSE: 0.104094\n",
      "AUC: 0.865407801418\n",
      "Valid batch loss (14 / 14): 0.41873827576637275\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.342548\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.45292139053344727\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.324756\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 15\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.490383\n",
      "MSE: 0.102291\n",
      "AUC: 0.860859929078\n",
      "Valid batch loss (14 / 14): 0.37998914718627935\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.32528\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.47876545786857605\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.32432\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 16\n",
      "TEST:\n",
      "Accuracy: 0.872832369942\n",
      "CrossEntropy: 0.480487\n",
      "MSE: 0.098101\n",
      "AUC: 0.860487588652\n",
      "Valid batch loss (14 / 14): 0.22462823987007143\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.335162\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.10181330889463425\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.322633\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 17\n",
      "TEST:\n",
      "Accuracy: 0.865895953757\n",
      "CrossEntropy: 0.488152\n",
      "MSE: 0.101058\n",
      "AUC: 0.861586879433\n",
      "Valid batch loss (14 / 14): 0.31843948364257817\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.348446\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.59451192617416385\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.316436\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 18\n",
      "TEST:\n",
      "Accuracy: 0.875144508671\n",
      "CrossEntropy: 0.466617\n",
      "MSE: 0.0950679\n",
      "AUC: 0.860735815603\n",
      "Valid batch loss (14 / 14): 0.40591907501220703\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.338511\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.41010609269142153\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.312666\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 19\n",
      "TEST:\n",
      "Accuracy: 0.868208092486\n",
      "CrossEntropy: 0.482573\n",
      "MSE: 0.101146\n",
      "AUC: 0.868475177305\n",
      "Valid batch loss (14 / 14): 0.59627467393875127\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.357253\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.64209860563278256\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.314781\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 20\n",
      "TEST:\n",
      "Accuracy: 0.868208092486\n",
      "CrossEntropy: 0.482573\n",
      "MSE: 0.101146\n",
      "AUC: 0.868475177305\n",
      "Valid batch loss (14 / 14): 0.36349558830261236\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.354366\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.11047765612602234\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.311399\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 21\n",
      "TEST:\n",
      "Accuracy: 0.870520231214\n",
      "CrossEntropy: 0.475047\n",
      "MSE: 0.0990322\n",
      "AUC: 0.868067375887\n",
      "Valid batch loss (14 / 14): 0.36228069663047796\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.336677\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.08195926994085312\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.306288\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 22\n",
      "TEST:\n",
      "Accuracy: 0.872832369942\n",
      "CrossEntropy: 0.477075\n",
      "MSE: 0.0990055\n",
      "AUC: 0.867304964539\n",
      "Valid batch loss (14 / 14): 0.19304008781909943\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.327962\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.15164265036582947\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.304376\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 23\n",
      "TEST:\n",
      "Accuracy: 0.86936416185\n",
      "CrossEntropy: 0.480752\n",
      "MSE: 0.0996475\n",
      "AUC: 0.866719858156\n",
      "Valid batch loss (14 / 14): 0.33791083097457886\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.335432\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.12873649597167976\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.305972\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 24\n",
      "TEST:\n",
      "Accuracy: 0.86936416185\n",
      "CrossEntropy: 0.480752\n",
      "MSE: 0.0996475\n",
      "AUC: 0.866719858156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid batch loss (14 / 14): 0.34099802374839784\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.332639\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.12238667160272598\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.304144\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 25\n",
      "TEST:\n",
      "Accuracy: 0.872832369942\n",
      "CrossEntropy: 0.482091\n",
      "MSE: 0.0998336\n",
      "AUC: 0.866994680851\n",
      "Valid batch loss (14 / 14): 0.45571476221084595\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.340725\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.79413324594497687\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.30156\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 26\n",
      "TEST:\n",
      "Accuracy: 0.870520231214\n",
      "CrossEntropy: 0.479362\n",
      "MSE: 0.0988485\n",
      "AUC: 0.86649822695\n",
      "Valid batch loss (14 / 14): 0.33774080872535706\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.348291\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.19844545423984528\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.306267\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 27\n",
      "TEST:\n",
      "Accuracy: 0.870520231214\n",
      "CrossEntropy: 0.479362\n",
      "MSE: 0.0988485\n",
      "AUC: 0.86649822695\n",
      "Valid batch loss (14 / 14): 0.24637357890605927\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.353412\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.30581408739089966\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.303681\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "EVAL TRAIN\n",
      "Accuracy: 0.88901734104\n",
      "CrossEntropy: 0.405412\n",
      "MSE: 0.0835153\n",
      "AUC: 0.900087582533\n",
      "Valid batch loss (109 / 109): 0.67267471551895146\n",
      "Dump summary valid\n",
      "Done\n",
      "EVAL TEST\n",
      "Accuracy: 0.870520231214\n",
      "CrossEntropy: 0.479362\n",
      "MSE: 0.0988485\n",
      "AUC: 0.86649822695\n",
      "Valid batch loss (14 / 14): 0.48398897051811223\n",
      "Dump summary valid\n",
      "Done\n",
      "Final train loss: 0.308231\n",
      "Final test loss: 0.346516\n"
     ]
    }
   ],
   "source": [
    "# XENT TRAIN\n",
    "\n",
    "train_config = {\"learning_rate\": 1e-3,\n",
    "                \"batch_size\": 64,\n",
    "                \"pair_loss_stretch\": 0.5,\n",
    "                \"rnn_input_keep_prob\": 0.5,\n",
    "                \"rnn_state_keep_prob\": 0.5}\n",
    "train(graph, data_train, data_valid, train_config, restore=None, loss_type=\"xent\", model=\"purpose\", max_drops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Eval scores\n",
      "Valid batch loss (1330 / 1330): 0.49086010456085205\n",
      "Dump summary train\n",
      "Done\n",
      "Init loss: 0.575503\n",
      "Epoch: 1\n",
      "TEST:\n",
      "Accuracy: 0.870520231214\n",
      "CrossEntropy: 0.479362\n",
      "MSE: 0.0988485\n",
      "AUC: 0.86649822695\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.5901160240173345\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.600389\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.40227848291397095\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.406848\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 2\n",
      "TEST:\n",
      "Accuracy: 0.828901734104\n",
      "CrossEntropy: 0.567877\n",
      "MSE: 0.123474\n",
      "AUC: 0.866241134752\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41793453693389894\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.407786\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.38040941953659067\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.393876\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 3\n",
      "TEST:\n",
      "Accuracy: 0.825433526012\n",
      "CrossEntropy: 0.5644\n",
      "MSE: 0.123487\n",
      "AUC: 0.867136524823\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.42591813206672673\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.409306\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.38862949609756474\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.390121\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 4\n",
      "TEST:\n",
      "Accuracy: 0.82774566474\n",
      "CrossEntropy: 0.565115\n",
      "MSE: 0.123012\n",
      "AUC: 0.868005319149\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40376731753349304\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.414153\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.40620189905166626\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.386939\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 5\n",
      "TEST:\n",
      "Accuracy: 0.825433526012\n",
      "CrossEntropy: 0.5669\n",
      "MSE: 0.124008\n",
      "AUC: 0.865842198582\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39645794034004215\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.410695\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.37441182136535645\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.384518\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 6\n",
      "TEST:\n",
      "Accuracy: 0.824277456647\n",
      "CrossEntropy: 0.558304\n",
      "MSE: 0.122496\n",
      "AUC: 0.870008865248\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43184840679168726\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.41198\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.40000635385513306\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.380743\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 7\n",
      "TEST:\n",
      "Accuracy: 0.824277456647\n",
      "CrossEntropy: 0.555316\n",
      "MSE: 0.121461\n",
      "AUC: 0.868563829787\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40729662775993347\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.411397\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.35749548673629765\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.377966\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 8\n",
      "TEST:\n",
      "Accuracy: 0.832369942197\n",
      "CrossEntropy: 0.546432\n",
      "MSE: 0.118899\n",
      "AUC: 0.866631205674\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41453900933265686\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.418502\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.35924717783927923\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.374958\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 9\n",
      "TEST:\n",
      "Accuracy: 0.832369942197\n",
      "CrossEntropy: 0.554596\n",
      "MSE: 0.12098\n",
      "AUC: 0.86905141844\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40925985574722297\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.412181\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.37912309169769287\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.372543\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 10\n",
      "TEST:\n",
      "Accuracy: 0.834682080925\n",
      "CrossEntropy: 0.565271\n",
      "MSE: 0.123343\n",
      "AUC: 0.862331560284\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40524506568908694\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.407741\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.36826807260513306\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.370599\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 11\n",
      "TEST:\n",
      "Accuracy: 0.840462427746\n",
      "CrossEntropy: 0.536982\n",
      "MSE: 0.116219\n",
      "AUC: 0.868333333333\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.46090263128280647\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.410306\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.36618134379386986\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.366616\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 12\n",
      "TEST:\n",
      "Accuracy: 0.834682080925\n",
      "CrossEntropy: 0.546753\n",
      "MSE: 0.119075\n",
      "AUC: 0.864592198582\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43165886402130127\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.422212\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33733350038528446\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.36674\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 13\n",
      "TEST:\n",
      "Accuracy: 0.834682080925\n",
      "CrossEntropy: 0.546753\n",
      "MSE: 0.119075\n",
      "AUC: 0.864592198582\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43641626834869385\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.414775\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34967696666717535\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.365607\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 14\n",
      "TEST:\n",
      "Accuracy: 0.838150289017\n",
      "CrossEntropy: 0.540567\n",
      "MSE: 0.116696\n",
      "AUC: 0.86045212766\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39723348617553714\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.419506\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.35749602317810066\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.363059\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 15\n",
      "TEST:\n",
      "Accuracy: 0.832369942197\n",
      "CrossEntropy: 0.543433\n",
      "MSE: 0.117566\n",
      "AUC: 0.858670212766\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43919950723648075\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.416711\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.37105026841163635\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.360568\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 16\n",
      "TEST:\n",
      "Accuracy: 0.833526011561\n",
      "CrossEntropy: 0.558964\n",
      "MSE: 0.121795\n",
      "AUC: 0.861276595745\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41629606485366823\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.41116\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.36191204190254213\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.358144\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 17\n",
      "TEST:\n",
      "Accuracy: 0.842774566474\n",
      "CrossEntropy: 0.547853\n",
      "MSE: 0.117988\n",
      "AUC: 0.859459219858\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43678700923919683\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.42131\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.35216346383094795\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.356763\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 18\n",
      "TEST:\n",
      "Accuracy: 0.838150289017\n",
      "CrossEntropy: 0.539219\n",
      "MSE: 0.115644\n",
      "AUC: 0.857136524823\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41971868276596077\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.412677\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33009356260299685\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.354149\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 19\n",
      "TEST:\n",
      "Accuracy: 0.840462427746\n",
      "CrossEntropy: 0.544981\n",
      "MSE: 0.116967\n",
      "AUC: 0.853306737589\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43744301795959474\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.416721\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.35391753911972046\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.352266\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 20\n",
      "TEST:\n",
      "Accuracy: 0.826589595376\n",
      "CrossEntropy: 0.557934\n",
      "MSE: 0.121572\n",
      "AUC: 0.855576241135\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41247493028640747\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.415738\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34877026081085205\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.351348\n",
      "Dump model model_dash/model.chkpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21\n",
      "TEST:\n",
      "Accuracy: 0.82774566474\n",
      "CrossEntropy: 0.566888\n",
      "MSE: 0.123214\n",
      "AUC: 0.850088652482\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.42253243923187256\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420457\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34480434656143196\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.351102\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 22\n",
      "TEST:\n",
      "Accuracy: 0.846242774566\n",
      "CrossEntropy: 0.53558\n",
      "MSE: 0.114143\n",
      "AUC: 0.853333333333\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.44033467769622817\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.421782\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34152123332023627\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.348896\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 23\n",
      "TEST:\n",
      "Accuracy: 0.847398843931\n",
      "CrossEntropy: 0.536861\n",
      "MSE: 0.113592\n",
      "AUC: 0.847810283688\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.37375009059906006\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.416586\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33153617382049564\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.347326\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 24\n",
      "TEST:\n",
      "Accuracy: 0.843930635838\n",
      "CrossEntropy: 0.529592\n",
      "MSE: 0.112346\n",
      "AUC: 0.85509751773\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39705011248588567\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.410322\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33001643419265747\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.346762\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 25\n",
      "TEST:\n",
      "Accuracy: 0.842774566474\n",
      "CrossEntropy: 0.54334\n",
      "MSE: 0.115461\n",
      "AUC: 0.849858156028\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40435639023780825\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.414625\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.32898175716400146\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.345402\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 26\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.537384\n",
      "MSE: 0.113385\n",
      "AUC: 0.85\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.46603584289550783\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.421966\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33178359270095825\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.343909\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 27\n",
      "TEST:\n",
      "Accuracy: 0.840462427746\n",
      "CrossEntropy: 0.548337\n",
      "MSE: 0.116936\n",
      "AUC: 0.845106382979\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.44137904047966003\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.438625\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34543311595916754\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.342576\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 28\n",
      "TEST:\n",
      "Accuracy: 0.840462427746\n",
      "CrossEntropy: 0.544941\n",
      "MSE: 0.116265\n",
      "AUC: 0.84780141844\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40527302026748663\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.417719\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33519014716148376\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.341386\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 29\n",
      "TEST:\n",
      "Accuracy: 0.846242774566\n",
      "CrossEntropy: 0.542004\n",
      "MSE: 0.115324\n",
      "AUC: 0.845354609929\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39817678928375244\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.413785\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34240758419036865\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.340917\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 30\n",
      "TEST:\n",
      "Accuracy: 0.848554913295\n",
      "CrossEntropy: 0.538045\n",
      "MSE: 0.114348\n",
      "AUC: 0.84570035461\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39596676826477057\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.431335\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33317595720291146\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.339557\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 31\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.533392\n",
      "MSE: 0.11285\n",
      "AUC: 0.848173758865\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.40029513835906984\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420032\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33042502403259287\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.338476\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 32\n",
      "TEST:\n",
      "Accuracy: 0.840462427746\n",
      "CrossEntropy: 0.546395\n",
      "MSE: 0.116817\n",
      "AUC: 0.845283687943\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.41402977705001837\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.424195\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34682196378707886\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.337874\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 33\n",
      "TEST:\n",
      "Accuracy: 0.842774566474\n",
      "CrossEntropy: 0.54736\n",
      "MSE: 0.1166\n",
      "AUC: 0.847659574468\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.44980925321578985\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.425225\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.31245106458663944\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.336273\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 34\n",
      "TEST:\n",
      "Accuracy: 0.849710982659\n",
      "CrossEntropy: 0.536522\n",
      "MSE: 0.113557\n",
      "AUC: 0.844867021277\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.45568534731864936\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.427022\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.31809514760971076\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.33632\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 35\n",
      "TEST:\n",
      "Accuracy: 0.849710982659\n",
      "CrossEntropy: 0.536522\n",
      "MSE: 0.113557\n",
      "AUC: 0.844867021277\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.44827258586883545\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.425186\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.31409251689910895\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.335557\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 36\n",
      "TEST:\n",
      "Accuracy: 0.847398843931\n",
      "CrossEntropy: 0.539034\n",
      "MSE: 0.114403\n",
      "AUC: 0.844113475177\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.42606222629547126\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420129\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33142158389091497\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.333893\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 37\n",
      "TEST:\n",
      "Accuracy: 0.850867052023\n",
      "CrossEntropy: 0.53263\n",
      "MSE: 0.112662\n",
      "AUC: 0.848617021277\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.45054921507835394\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.422105\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.32336717844009446\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.333311\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 38\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43664368987083435\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420276\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.32117617130279546\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.333821\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 39\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.43192186951637276\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.425521\n",
      "TRAIN:\n",
      "Learning rate: 1e-05\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.34459692239761356\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.333453\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 40\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39392450451850894\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420137\n",
      "TRAIN:\n",
      "Learning rate: 1e-05\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.32426756620407104\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.334312\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 41\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Eval scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid batch loss (22 / 22): 0.43106287717819214\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.423863\n",
      "TRAIN:\n",
      "Learning rate: 1e-05\n",
      "Eval scores\n",
      "Sort samples for pair loss\n",
      "Train batch loss (1330 / 1330): 0.33574768900871277\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.333705\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "EVAL TRAIN\n",
      "Accuracy: 0.939161849711\n",
      "CrossEntropy: 0.299511\n",
      "MSE: 0.0506157\n",
      "AUC: 0.979719071218\n",
      "Eval scores\n",
      "Valid batch loss (1330 / 1330): 0.33451214432716374\n",
      "Dump summary valid\n",
      "Done\n",
      "EVAL TEST\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Eval scores\n",
      "Valid batch loss (22 / 22): 0.39902043342590333\n",
      "Dump summary valid\n",
      "Done\n",
      "Final train loss: 0.335116\n",
      "Final test loss: 0.422574\n"
     ]
    }
   ],
   "source": [
    "# AUC TRAIN\n",
    "train_config = {\"learning_rate\": 1e-4,\n",
    "                \"batch_size\": 64,\n",
    "                \"pair_loss_stretch\": 0.2,\n",
    "                \"rnn_input_keep_prob\": 0.5,\n",
    "                \"rnn_state_keep_prob\": 0.5}\n",
    "train(graph, data_train, data_valid, train_config, restore=\"purpose\", loss_type=\"pair\", model=\"purpose\", max_drops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Valid batch loss (109 / 109): 0.67462956905364993\n",
      "Dump summary train\n",
      "Done\n",
      "Init loss: 0.322369\n",
      "Epoch: 1\n",
      "TEST:\n",
      "Accuracy: 0.852023121387\n",
      "CrossEntropy: 0.53469\n",
      "MSE: 0.112879\n",
      "AUC: 0.842739361702\n",
      "Valid batch loss (14 / 14): 0.49620071053504944\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.458627\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.43660894036293036\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.206097\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 2\n",
      "TEST:\n",
      "Accuracy: 0.854335260116\n",
      "CrossEntropy: 0.575364\n",
      "MSE: 0.110863\n",
      "AUC: 0.844423758865\n",
      "Valid batch loss (14 / 14): 0.34288141131401065\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.373056\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.46801766753196716\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.182122\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 3\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.60662\n",
      "MSE: 0.112372\n",
      "AUC: 0.843209219858\n",
      "Valid batch loss (14 / 14): 0.47430264949798584\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.420549\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.17612431943416595\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.184894\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 4\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.60662\n",
      "MSE: 0.112372\n",
      "AUC: 0.843209219858\n",
      "Valid batch loss (14 / 14): 0.33701348304748535\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.395695\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.041368402540683746\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.181121\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 5\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.592406\n",
      "MSE: 0.109007\n",
      "AUC: 0.849760638298\n",
      "Valid batch loss (14 / 14): 0.33858248591423035\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.430515\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.34405937790870667\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.182413\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 6\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.592406\n",
      "MSE: 0.109007\n",
      "AUC: 0.849760638298\n",
      "Valid batch loss (14 / 14): 0.27998045086860657\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.418248\n",
      "TRAIN:\n",
      "Learning rate: 0.001\n",
      "Train batch loss (109 / 109): 0.05944783240556717\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.185972\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 7\n",
      "TEST:\n",
      "Accuracy: 0.861271676301\n",
      "CrossEntropy: 0.592406\n",
      "MSE: 0.109007\n",
      "AUC: 0.849760638298\n",
      "Valid batch loss (14 / 14): 0.25678521394729614\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.40652\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.26760980486869813\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.169374\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 8\n",
      "TEST:\n",
      "Accuracy: 0.864739884393\n",
      "CrossEntropy: 0.599487\n",
      "MSE: 0.108286\n",
      "AUC: 0.84954787234\n",
      "Valid batch loss (14 / 14): 0.33508911728858956\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.401367\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.05570065602660179\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.166911\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 9\n",
      "TEST:\n",
      "Accuracy: 0.865895953757\n",
      "CrossEntropy: 0.603509\n",
      "MSE: 0.108539\n",
      "AUC: 0.84765070922\n",
      "Valid batch loss (14 / 14): 0.18640151619911194\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.404324\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.12026286125183105\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.153433\n",
      "Dump model model_dash/model.chkpt\n",
      "Epoch: 10\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.608493\n",
      "MSE: 0.109276\n",
      "AUC: 0.847367021277\n",
      "Valid batch loss (14 / 14): 0.34837892651557927\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.407096\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.060224663466215134\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.159271\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 11\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.608493\n",
      "MSE: 0.109276\n",
      "AUC: 0.847367021277\n",
      "Valid batch loss (14 / 14): 0.16583479940891266\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.410159\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.07200497388839722\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.156653\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "Epoch: 12\n",
      "TEST:\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.608493\n",
      "MSE: 0.109276\n",
      "AUC: 0.847367021277\n",
      "Valid batch loss (14 / 14): 0.14887888729572296\n",
      "Dump summary valid\n",
      "Done\n",
      "Test loss: 0.441789\n",
      "TRAIN:\n",
      "Learning rate: 0.0001\n",
      "Train batch loss (109 / 109): 0.16436816751956946\n",
      "Dump summary train\n",
      "Done\n",
      "Train loss: 0.154695\n",
      "Revert\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "EVAL TRAIN\n",
      "Accuracy: 0.960115606936\n",
      "CrossEntropy: 0.152551\n",
      "MSE: 0.0303626\n",
      "AUC: 0.988542932631\n",
      "Valid batch loss (109 / 109): 0.069311730563640635\n",
      "Dump summary valid\n",
      "Done\n",
      "EVAL TEST\n",
      "Accuracy: 0.863583815029\n",
      "CrossEntropy: 0.608493\n",
      "MSE: 0.109276\n",
      "AUC: 0.847367021277\n",
      "Valid batch loss (14 / 14): 0.52443987131118775\n",
      "Dump summary valid\n",
      "Done\n",
      "Final train loss: 0.14988\n",
      "Final test loss: 0.455863\n"
     ]
    }
   ],
   "source": [
    "# XENT TRAIN\n",
    "train_config = {\"learning_rate\": 1e-3,\n",
    "                \"batch_size\": 64,\n",
    "                \"pair_loss_stretch\": 0.5,\n",
    "                \"rnn_input_keep_prob\": 0.5,\n",
    "                \"rnn_state_keep_prob\": 0.5}\n",
    "train(graph, data_train, data_valid, train_config, restore=\"purpose\", loss_type=\"xent\", model=\"purpose\", max_drops=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "Converted 6 variables to const ops.\n",
      "Exported to graph\n",
      "Input tensors: [<tf.Tensor 'features:0' shape=(?, ?, 649) dtype=float32>, <tf.Tensor 'lengths:0' shape=<unknown> dtype=int32>]\n",
      "Output tensor: Tensor(\"Softmax:0\", shape=(?, 2), dtype=float32)\n",
      "Info tensors: []\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    info(\"Load model\")\n",
    "    graph.load(MODEL_FILENAME, model=\"purpose\")\n",
    "    graph.export(OUTPUT_FROZEN_GRAPH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_roc(graph, data, model=\"purpose\"):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        graph.load(MODEL_FILENAME, model=model)\n",
    "        scores = graph.predict(data, model=model)\n",
    "        if model == \"accent\":\n",
    "            scores, labels = split_word_scores(scores, data)\n",
    "        else:\n",
    "            labels = np.array([sample.label for sample in data])\n",
    "    roc = get_roc(scores[:, 1], labels, th_range=(np.min(scores), np.max(scores)))\n",
    "    info(\"AUC:\", get_auc(roc))\n",
    "    plot_roc(roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "AUC: 0.988542932631\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_roc(graph, data_train, model=\"purpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "AUC: 0.847367021277\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_roc(graph, data_valid, model=\"purpose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_dash/model.chkpt\n",
      "AUC: 0.852241870323\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_roc(graph, data_test, model=\"purpose\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
